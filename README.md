# CONVO - A Curious Conversational LLM
LLMs are good at answering questions. They should be able to ask them as well.
This implements a PyTorch-based implementation of a language model that generates questions using 
the Hugging Face Transformers library. We'll use a pre-trained model and fine-tune it for question generation. 

Nearly all of the initial code was generated by Claude.ai in response to my prompts.

## The earliest implementation does the following:

1. We use the T5 model, which is suitable for various text-to-text tasks, including question generation.
2. We load a pre-trained T5 model and tokenizer from Hugging Face.
3. We use a subset of the SQuAD dataset for fine-tuning our model on question generation.
4. We preprocess the data to format it for our task: generating questions given a context and answer.
5. We set up a training loop to fine-tune the model on our dataset.
6. Finally, we provide a function to generate questions given a context and answer.

## To use this implementation:

1. Install the required libraries: `pip install torch transformers datasets`
2. Copy the code into a Python file or Jupyter notebook.
3. Run the code to fine-tune the model and generate questions.

Note that this is a basic implementation and may require further optimization for production use. You might want to consider:

1. Using a larger subset of the dataset or a custom dataset for better results.
2. Experimenting with different model architectures or sizes (e.g., T5-base or T5-large).
3. Implementing evaluation metrics to assess the quality of generated questions.
4. Adding early stopping and model checkpointing during training.

Turning Claude's first pass into a command line conversational engine:
We've created a **QuestionAnswerCLI** class that encapsulates the model loading and question generation functionality.
The **interactive_session method** in this class handles the continuous Q&A interaction with the user.
We've added a **train_model** function that allows users to train the model on their own dataset.
The **main** function sets up command-line argument parsing to either train the model or start an interactive session.

T5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

## To start an interactive Q&A session
```
python qa_cli.py --model ./qa_model
```
## During the interactive session:

Enter an initial context when prompted.
The AI will generate questions based on the context.
Answer the questions or type 'exit' to end the session.

## Unit Tests
This set of unit tests covers the main functionality of the qa_cli.py script. Here's a breakdown of the test cases:

**TestQuestionAnswerCLI**

Tests the initialization of the **QuestionAnswerCLI** class.
Tests the **generate_question** method.
Tests the **interactive_session** method.


**TestTrainModel**

Tests the **train_model** function.


**TestMain**

Tests the **main** function for both training and interactive modes.



### To run these tests:

Save the test code above in a file named test_qa_cli.py in the same directory as your qa_cli.py file.
Make sure you have the unittest module installed (it comes with Python standard library).
Run the tests using the command: python -m unittest test_qa_cli.py

These tests use mocking extensively to avoid actual model loading and training, 
which would be time-consuming and unnecessary for unit testing. The tests focus on ensuring that:

The correct methods are called with the expected arguments.
The flow of the program is correct in different scenarios.
The interactive session behaves as expected.

### Versions
- **qa_cli.py** First approximation
- **qa_cli_v1.py** Avoid single word questions
- **qa_cli_v2.py** Hybrid with pre-trained model and rule-based backup
- **qa_cli_v3.py** Add sentiment analysis
- **qa_cli_v4.py** Keeps track of questions, doesn't repeat.
- **qa_cli_v5.py** performs sentiment analysis on questions, selects question with the greatest absolute value 
- **qa_cli_v6.py** tries to filter out nonsense questions
- **qa_cli_v7.py** retries downloads
- **qa_cli_v8.py** first attempt at fully conversational mode.
- **combined_qa_cli.py** Concurrent multi-user, fine-tunes individual user models.
- **abstract_qa_service.py** Concurrent, multi-user, fine-tunes individual user models, can use other models such as BERT and GPT2. See below for a description.
## Creating Q&A datasets and training a model for conversationality
Q&A data was downloaded from [COQA](https://stanfordnlp.github.io/coqa/) and processed into the following format:
```
[{"context": <some block of text here>,
 "questions": [question_1, question_2, ... question_n]},
 {"context":...}]
```
The script **fine_tune_t5.py** takes JSON files in this format and tunes the T5 model for conversationality. 
Invoke like so:
```
python fine_tune_t5.py --train_data path/to/train.json --val_data path/to/val.json --model_name t5-base --num_epochs 3 --batch_size 8 --learning_rate 5e-5 --output_dir ./fine_tuned_qa_model

## Abstract QA Service
This implementation of the EnhancedMultiUserQuestionAnswerCLI supports multiple concurrent conversations. Here are the key features that enable this functionality:

1. User-specific data storage:
   The class maintains a dictionary `self.users` where each user's data (model, knowledge base, and conversation) is stored separately, keyed by user ID.

2. Thread-safe user creation:
   The `get_or_create_user` method uses a threading lock (`self.user_lock`) to ensure thread-safe creation and retrieval of user data.

3. Concurrent session handling:
   The `run_concurrent_sessions` method uses a ThreadPoolExecutor to process inputs from multiple users concurrently:

   ```python
   def run_concurrent_sessions(self, user_inputs: Dict[str, List[str]]):
       with ThreadPoolExecutor() as executor:
           futures = []
           for user_id, inputs in user_inputs.items():
               for user_input in inputs:
                   futures.append(executor.submit(self.process_user_input, user_id, user_input))
   ```

4. User-specific model instances:
   Each user has their own instance of the language model, allowing for personalized responses and fine-tuning.

5. Separate conversation histories:
   Each user's conversation history is maintained separately in their respective `Conversation` object.

6. Individual knowledge bases:
   Each user has their own `UserKnowledgeBase`, allowing for personalized knowledge accumulation and retrieval.

While the main function in this script is set up for a single-user interactive session, the underlying `EnhancedMultiUserQuestionAnswerCLI` class is designed to handle multiple users concurrently. To fully utilize this capability, we will need to modify the main function or create a new entry point that interacts with multiple users simultaneously, possibly through a web interface or a multi-client network application.

Additionally, the next version should support only saving deltas to the user-specific model in order to save spaceIn summary, yes, this implementation supports multiple concurrent conversations, although the provided main function doesn't demonstrate this capability. The core class is built to handle multiple users efficiently and concurrently.
```.
